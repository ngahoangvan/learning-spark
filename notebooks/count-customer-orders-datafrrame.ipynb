{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 20:18:55 WARN Utils: Your hostname, Ngas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.115 instead (on interface en0)\n",
      "24/11/25 20:18:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/25 20:18:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CustomerOrders\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"item_id\", IntegerType(), False),\n",
    "    StructField(\"amount_spent\", FloatType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+\n",
      "|customer_id|item_id|amount_spent|\n",
      "+-----------+-------+------------+\n",
      "|         44|   8602|       37.19|\n",
      "|         35|   5368|       65.89|\n",
      "|          2|   3391|       40.64|\n",
      "|         47|   6694|       14.98|\n",
      "|         29|    680|       13.08|\n",
      "|         91|   8900|       24.59|\n",
      "|         70|   3959|       68.68|\n",
      "|         85|   1733|       28.53|\n",
      "|         53|   9900|       83.55|\n",
      "|         14|   1505|        4.32|\n",
      "|         51|   3378|        19.8|\n",
      "|         42|   6926|       57.77|\n",
      "|          2|   4424|       55.77|\n",
      "|         79|   9291|       33.17|\n",
      "|         50|   3901|       23.57|\n",
      "|         20|   6633|        6.49|\n",
      "|         15|   6148|       65.53|\n",
      "|         44|   8331|       99.19|\n",
      "|          5|   3505|       64.18|\n",
      "|         48|   5539|       32.42|\n",
      "+-----------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.schema(schema).csv(\"../data/customer-orders.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|total_spent|\n",
      "+-----------+-----------+\n",
      "|         31|    4765.05|\n",
      "|         85|    5503.43|\n",
      "|         65|    5140.35|\n",
      "|         53|     4945.3|\n",
      "|         78|    4524.51|\n",
      "|         34|     5330.8|\n",
      "|         81|    5112.71|\n",
      "|         28|    5000.71|\n",
      "|         76|    4904.21|\n",
      "|         27|    4915.89|\n",
      "|         26|     5250.4|\n",
      "|         44|    4756.89|\n",
      "|         12|    4664.59|\n",
      "|         91|    4642.26|\n",
      "|         22|    5019.45|\n",
      "|         93|    5265.75|\n",
      "|         47|     4316.3|\n",
      "|          1|     4958.6|\n",
      "|         52|    5245.06|\n",
      "|         13|    4367.62|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_by_customer = df.groupBy(\"customer_id\").agg(func.round(func.sum(\"amount_spent\"), 2).alias(\"total_spent\"))\n",
    "total_by_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|total_spent|\n",
      "+-----------+-----------+\n",
      "|         45|    3309.38|\n",
      "|         79|    3790.57|\n",
      "|         96|    3924.23|\n",
      "|         23|    4042.65|\n",
      "|         99|    4172.29|\n",
      "|         75|     4178.5|\n",
      "|         36|    4278.05|\n",
      "|         98|    4297.26|\n",
      "|         47|     4316.3|\n",
      "|         77|    4327.73|\n",
      "|         13|    4367.62|\n",
      "|         48|    4384.33|\n",
      "|         49|     4394.6|\n",
      "|         94|    4475.57|\n",
      "|         67|    4505.79|\n",
      "|         50|    4517.27|\n",
      "|         78|    4524.51|\n",
      "|          5|    4561.07|\n",
      "|         57|     4628.4|\n",
      "|         83|     4635.8|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorted by total_spent\n",
    "total_by_customer_sorted = total_by_customer.sort(\"total_spent\")\n",
    "total_by_customer_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
